---
title: "Building a Predictive Model on Covid-19 Cases"
subtitle: "Using Machine Learning Models to Predict Covid-19 Outbreak Risk in Californian Cities"
author: "Lyndsey Umsted"
format: html
editor: visual
---

# Introduction

This project is intended to build and deploy a machine learning classification model which categorizes California zip codes into low, moderate, or high risk for Covid-19 outbreaks by the end of the year of 2020. Risk predictions of Covid-19 were made using 30 different predictors including socioeconomic factors, and abiotic factors such as temperature, and pollution indicators. This project originates from a personal interest in public health and previous studying done on disease dynamics.

#### Packages and Libraries:

```{r, message = FALSE}
library(corrplot)  # for the correlation plot
library(discrim)  # for linear discriminant analysis
library(corrr)   # for calculating correlation
library(knitr)   # to help with the knitting process
library(MASS)    # to assist with the markdown processes
library(tidyverse)   # using tidyverse and tidymodels for this project mostly
library(tidymodels)
library(ggplot2)   # for most of our visualizations
tidymodels_prefer()
library(ISLR) 
library(ISLR2) 
library(poissonreg)
library(klaR) # for naive bayes
library(randomForest)
```

## What is Covid-19?

Coronavirus disease (COVID-19) is an infectious disease caused by the SARS-CoV-2 virus. Most people infected with the virus will experience mild to moderate respiratory illness and recover without requiring special treatment. Some people become seriously ill and require medical attention. Older people and those with underlying medical conditions are more likely to develop serious illness and hospitalizaton. COVID-19 can affect people of all ages, including death. The virus can spread from an infected person's mouth or nose in small liquid particles when they cough, sneeze, speak, sing or breathe. These particles range from larger respiratory droplets to smaller aerosols.

https://www.who.int/health-topics/coronavirus#tab=tab_1

## My Data

The data set used for this research project was provided to me by a faculty member, Dr. Andrew MacDonald, who I had the opportunity to work under this summer during an undergraduate research internship. Dr. MacDonald merged data from the *LA Times* on socioeconomic factors and food access for different Californian zip codes with data reflecting Covid-19 case counts, weather, and pollution levels from the same zip codes during the Covid-19 pandemic in 2020. The original data set contained 273 columns of data.

#### Predictor Variables:

1.  `Mean_Max`: Highest recorded temperature in degrees Celsius.
2.  `Urban`: Flag for urban tract.
3.  `PovertyRate`: Share of the tract population living with income at or below the Federal poverty thresholds for family size.
4.  `MedianFamilyIncome`: Tract median family income.
5.  `Traffic`: Traffic density, in vehicle-kilometers per hour per road length, within 150 meters of the census tract boundary.
6.  `Sold.Waste`: Sum of weighted solid waste sites and facilities (SWIS) within Â buffered distances to populated blocks of census tracts.
7.  `Asthma`: Age-adjusted rate of emergency department visits for asthma.
8.  `Low.Birth.Weight`: Percentage of low birth weights.
9.  `Cardiovascular.Disease`: Age-adjusted rate of emergency department visits for heart attacks per 10,000.
10. `Education`: Percent of population over 25 with less than a high school education.
11. `Linguistic.Isolation`: Percent limited English speaking households.
12. `Unemployment`: Percent of the population over the age of 16 that is unemployed and eligible for the labor force.
13. `under_10_.`: Percent of the population of the age 10 or younger.
14. `Age11_to_64_.`: Percent of the population between the ages 11 and 64.
15. `over_65_.`: Percent of the population of the age 65 or older.
16. `Hispanic_.`: Percent of the population that is Hispanic.
17. `White_.`: Percent of the population that is White.
18. `African_Am_.`: Percent of the population that is African American.
19. `Asian_Am_.:` Percent of the population that is Asian American.
20. `Native_Am_.`: Percent of the population that is Native American
21. `Other_ethnicity_.`: Percent of the population that belongs to an ethnic group not identified above.

## Research Questions

1.  What factors of a population have underlying effects on the number of Covid-19 Cases?
2.  What kinds of populations are at higher risk for Covid-19 outbreaks?
3.  Is there evidence of socioeconomic correlation with higher Covid-19 rates?
4.  Is there evidence of pollution level effects on Covid-19 rates?

# Exploratory Data Analysis

The original data set used for this project contained 55,620 observations of 268 variables. In the beginning stages of this project, this data set was tidied, cleaned, and reduced to fewer variables. In the following sections we'll explore the distribution of our response variable "outbreak risk" and look at visualizations of key predictor distributions and relationships between predictors and case counts of Covid-19.

## Loading and Exploring Raw Data

The pandemic data set is stored in a .csv format and can be read in from the directory:

```{r, echo=FALSE}
setwd("C:/Users/18586/Desktop/PSTAT 131/PSTAT-131-final-project")
pandemic_weekly_og <- read.csv("data/pandemic_weekly.csv")

# changing naming to shorten
pandemic_weekly_og$confirmed_cases <- pandemic_weekly_og$confirmed_cases_fill
pandemic_weekly_og$new_cases <- pandemic_weekly_og$new_cases_fill

```

Dimensions of the data:

```{r, echo=FALSE}
dim(pandemic_weekly_og)
```

This is a very large data set with 270 variables, all of which are not necessary. The variable labeled "confirmed_cases" will be the focus of our predictions. This column contains the cumulative number of positive Covid-19 cases per week in each zip-code during the 2020 year. The data set also contains a variable labeled "new_cases" which contains the new number of positive Covid-19 cases per week during the 2020 year. We can explore these two variables:

#### Missing Values:

```{r}
load("visuals/og_missing_plot.rda")
og_missing_plot
```

As we can see, there are not a lot of missing values in our original data for these variables, thus a solution to the missing values problem is to just remove them from our data:

```{r}
load("models/pandemic_cum_sub.rda")
pandemic_cum <- pandemic_cum_sub %>%
  na.omit()
```

#### Cumulative Cases throughout 2020:

```{r, echo=FALSE}
load("models/pandemic_cum.rda")
load("visuals/gg_cum.rda")
load("visuals/total_cases_cum.rda")
barplot(total_cases_cum$cumulative_cases, total_cases_cum$week, 
        names.arg = c(7:51), 
        ylab = "cumulative cases", 
        xlab = "week",
        main = "Cumulative Number of Covid-19 Cases per Week in 2020",
        col = "pink")
gg_cum
```

#### New Cases each week of 2020:

```{r, echo=FALSE}
load("visuals/gg_new.rda")
load("visuals/total_cases.rda")
barplot(total_cases$new_cases, total_cases$week, 
        names.arg = c(7:51), 
        ylab = "new cases", 
        xlab = "week",
        main = "New Number of Covid-19 Cases per Week in 2020",
        col = "pink")
gg_new
```

Here we can see that there was a rise in cases around weeks 26 to 35 which are the months June through August and then there is another spike following week 46 which is about halfway through November to the rest of the year right around the winter holidays.

## Converting Cumulative Cases into Proportions of Population for a Classification Model

My initial plan for this project was to build a regression model which predicted continuous case counts by the end of 2020 for each zip code. However, I decided to take this a step further and build a classification model which categorized zip codes into low, moderate, or high risk for a Covid-19 outbreak. Covid-19 cumulative case numbers are highly positively correlated with population so I decided to make a new column in the data set of the proportion of a population which has tested positive for Covid-19 by the end of the year 2020.

```{r, echo=FALSE}
boxplot(pandemic_cum$prop, horizontal = TRUE, main = "Distribution of Proportion of Covid-19 Cases in a Population",
        xlab = "Proportion of Covid-19 Cases in a Population", col = "light blue")
```

```{r, echo=FALSE}
hist(pandemic_cum$prop, breaks = 50, main = "Distribution of Proportion of Covid-19 Cases in a Population",
     xlab = "Proportion of Covid-19 Cases in a Population", col = "light blue")
```

#### Choosing Splits for Low, Moderate, and High Risk for Covid-19 Outbreak:

As I was choosing a suitable split for what would be considered low, moderate, or high risk I researched what is considered a "high" amount of cases. According to the World Health Organization, a 5% positive rate was considered a threshold for reopening stores and businesses during 2020. Thus, any proportions of Covid-19 cases in a population greater than 5% would be classified as high. For the low split, I decided that 2% would be a decent threshold to separate low and moderate risk. Following this, I then created a variable called "risk" where I classified proportions in each of these intervals into low, moderate, and high risk.

```{r, echo=FALSE}
# creating low, moderate, and high categories based on
# WHO suggestion

pandemic_cum$risk <- 
  cut(pandemic_cum$prop, breaks = 
        c(0, 0.02 , 0.05, 1), 
      labels = c("low", "moderate", "high")) # 0.03 and 0.07

# checking class imbalance
round(prop.table(table(pandemic_cum$risk)), 2)
```

There is some class imbalance with only 28% of the observations being categorized as high risk, and only 30% considered low risk, so all models will be stratified on this variable to avoid any prediction problems.

#### Correlation Between Variables:

```{r, echo=FALSE}
load("models/pandemic_cum.rda")
library(corrplot)
pandemic_cum %>% 
  dplyr::select(-c(prop)) %>%
  dplyr::select(is.numeric) %>% 
  cor() %>% 
  corrplot(diag = FALSE, 
           method = 'square')
```

This correlation plot tells us a lot of information about the linear relationships between our predictor variables. In particular I found great interest in the high correlations Poverty Rate and Median Family Income had with the following predictor variables: Asthma, Low Birth Weight, Cardiovascular Disease, Education, Linguistic Isolation, Unemployment, Percentage of Population being under 10 years of age, Percentage of Population being Hispanic, and Percentage of Population being White. Three of these variables include medical conditions such as Asthma, Low Birth Weight, and Cardiovascular Disease so I decided to visualize each of these conditions' relationships with Poverty Rate.

#### Mean Asthma and Poverty Rate

```{r, echo=FALSE}
load("visuals/pov_asthma_boxplot.rda")
pov_asthma_boxplot
```

```{r, echo=FALSE}
load("visuals/pov_asthma_gg.rda")
pov_asthma_gg
```

#### Mean Low Birth Weight and Poverty Rate

```{r, echo=FALSE}
load("visuals/pov_low_birth_boxplot.rda")
pov_low_birth_boxplot
```

```{r, echo=FALSE}
load("visuals/pov_low_birth_gg.rda")
pov_low_birth_gg
```

#### Mean Cardiovascular Disease and Poverty Rate:

```{r, echo=FALSE}
load("visuals/pov_cardio_boxplot.rda")
pov_cardio_boxplot
```

```{r, echo=FALSE}
load("visuals/pov_cardio_gg.rda")
pov_cardio_gg
```

#### Variable Importance:

```{r, echo=FALSE}
load("visuals/var_imp_plot.rda")
varImpPlot(model_spec_default, cex = 1, pch = 16, main = "Variable Importance Under Random Forest Model")
```

### Distribution of Risk Levels for Different Predictor Variables:

#### Hispanic:

```{r, echo=FALSE}
pandemic_cum %>% 
  dplyr::select("risk", "Hispanic_.") %>%
  mutate(Hispanic_. = cut(Hispanic_., breaks = 
                            seq(min(Hispanic_.), max(Hispanic_.), by = 5), 
                          include.lowest = TRUE)) %>%
  group_by(Hispanic_.) %>%
  na.omit(Hispanic_.) %>%
  ggplot(aes(Hispanic_.)) + 
  geom_bar(aes(fill = risk)) +
  scale_fill_manual(values = c("light blue", "pink", "light green")) +
  theme(axis.text.x = element_text(angle = 90))
```

#### Education:

```{r, echo=FALSE}
pandemic_cum %>% 
  dplyr::select("risk", "Education") %>%
  mutate(Education = cut(Education, breaks = 
                            seq(min(Education), max(Education), by = 2), 
                          include.lowest = TRUE)) %>%
  group_by(Education) %>%
  na.omit(Education) %>%
  ggplot(aes(Education)) + 
  geom_bar(aes(fill = risk)) +
  scale_fill_manual(values = c("light blue", "pink", "light green")) +
  theme(axis.text.x = element_text(angle = 90))

```

#### White:

```{r, echo=FALSE}
pandemic_cum %>% 
  dplyr::select("risk", "White_.") %>%
  mutate(White_. = cut(White_., breaks = 
                            seq(min(White_.), max(White_.), by = 2.5), 
                          include.lowest = TRUE)) %>%
  group_by(White_.) %>%
  na.omit(White_.) %>%
  ggplot(aes(White_.)) + 
  geom_bar(aes(fill = risk)) +
  scale_fill_manual(values = c("light blue", "pink", "light green")) +
  theme(axis.text.x = element_text(angle = 90))
```

#### Linguistic Isolation:

```{r, echo=FALSE}
pandemic_cum %>% 
  dplyr::select("risk", "Linguistic.Isolation") %>%
  mutate(Linguistic.Isolation = 
           cut(Linguistic.Isolation, 
               breaks = seq(min(Linguistic.Isolation), 
                            max(Linguistic.Isolation), by = 1), 
                          include.lowest = TRUE)) %>%
  group_by(Linguistic.Isolation) %>%
  na.omit(Linguistic.Isolation) %>%
  ggplot(aes(Linguistic.Isolation)) + 
  geom_bar(aes(fill = risk)) +
  scale_fill_manual(values = c("light blue", "pink", "light green")) +
  theme(axis.text.x = element_text(angle = 90))
```

#### Median Family Income:

```{r, echo=FALSE}
pandemic_cum %>% 
  dplyr::select("risk", "MedianFamilyIncome") %>%
  mutate(MedianFamilyIncome = cut(MedianFamilyIncome, 
                                  breaks = seq(min(MedianFamilyIncome), 
                                  max(MedianFamilyIncome), by = 5000), 
                          include.lowest = TRUE)) %>%
  group_by(MedianFamilyIncome) %>%
  na.omit(MedianFamilyIncome) %>%
  ggplot(aes(MedianFamilyIncome)) + 
  geom_bar(aes(fill = risk)) +
  scale_fill_manual(values = c("light blue", "pink", "light green")) +
  theme(axis.text.x = element_text(angle = 90))
```

#### Poverty Rate:

```{r, echo=FALSE}
pandemic_cum %>% 
  dplyr::select("risk", "PovertyRate") %>%
  mutate(PovertyRate = cut(PovertyRate, 
                                  breaks = seq(min(PovertyRate), 
                                  max(PovertyRate), by = 1), 
                          include.lowest = TRUE)) %>%
  group_by(PovertyRate) %>%
  na.omit(PovertyRate) %>%
  ggplot(aes(PovertyRate)) + 
  geom_bar(aes(fill = risk)) +
  scale_fill_manual(values = c("light blue", "pink", "light green")) +
  theme(axis.text.x = element_text(angle = 90))
```

#### Cardiovascular Disease:

```{r, echo=FALSE}
pandemic_cum %>% 
  dplyr::select("risk", "Cardiovascular.Disease") %>%
  mutate(Cardiovascular.Disease = cut(Cardiovascular.Disease, 
                                  breaks = seq(min(Cardiovascular.Disease), 
                                  max(Cardiovascular.Disease), by = 0.5), 
                          include.lowest = TRUE)) %>%
  group_by(Cardiovascular.Disease) %>%
  na.omit(Cardiovascular.Disease) %>%
  ggplot(aes(Cardiovascular.Disease)) + 
  geom_bar(aes(fill = risk)) +
  scale_fill_manual(values = c("light blue", "pink", "light green")) +
  theme(axis.text.x = element_text(angle = 90))
```

#### Unemployment:

```{r, echo=FALSE}
pandemic_cum %>% 
  dplyr::select("risk", "Unemployment") %>%
  mutate(Unemployment = cut(Unemployment, 
                                  breaks = seq(min(Unemployment), 
                                  max(Unemployment), by = 1), 
                          include.lowest = TRUE)) %>%
  group_by(Unemployment) %>%
  na.omit(Unemployment) %>%
  ggplot(aes(Unemployment)) + 
  geom_bar(aes(fill = risk)) +
  scale_fill_manual(values = c("light blue", "pink", "light green")) +
  theme(axis.text.x = element_text(angle = 90))
```

#### Under 10:

```{r, echo=FALSE}
pandemic_cum %>% 
  dplyr::select("risk", "under_10_.") %>%
  mutate(under_10_. = cut(under_10_., 
                                  breaks = seq(min(under_10_.), 
                                  max(under_10_.), by = 1), 
                          include.lowest = TRUE)) %>%
  group_by(under_10_.) %>%
  na.omit(under_10_.) %>%
  ggplot(aes(under_10_.)) + 
  geom_bar(aes(fill = risk)) +
  scale_fill_manual(values = c("light blue", "pink", "light green")) +
  theme(axis.text.x = element_text(angle = 90))
```

#### Over 65:

```{r, echo=FALSE}
pandemic_cum %>% 
  dplyr::select("risk", "over_65_.") %>%
  mutate(over_65_. = cut(over_65_., 
                                  breaks = seq(min(over_65_.), 
                                  max(over_65_.), by = 1), 
                          include.lowest = TRUE)) %>%
  group_by(over_65_.) %>%
  na.omit(over_65_.) %>%
  ggplot(aes(over_65_.)) + 
  geom_bar(aes(fill = risk)) +
  scale_fill_manual(values = c("light blue", "pink", "light green")) +
  theme(axis.text.x = element_text(angle = 90))
```

#### Max Temperature:

```{r, echo=FALSE}
pandemic_cum %>% 
  dplyr::select("risk", "Mean_Tmax") %>%
  mutate(Mean_Tmax = cut(Mean_Tmax, 
                                  breaks = seq(min(Mean_Tmax), 
                                  max(Mean_Tmax), by = 0.5), 
                          include.lowest = TRUE)) %>%
  group_by(Mean_Tmax) %>%
  na.omit(Mean_Tmax) %>%
  ggplot(aes(Mean_Tmax)) + 
  geom_bar(aes(fill = risk)) +
  scale_fill_manual(values = c("light blue", "pink", "light green")) +
  theme(axis.text.x = element_text(angle = 90))
```

## Data Splitting:

When fitting the pandemic data to my machine learning models, I need to split the data into a training and testing/validation set. This allows the models to be trained on a random subset of the data in order to build predictions. Once trained on the training set, each of the models are then used to create predictions on the testing/validation set in order to get a measure of the models' accuracies for making predictions on new data. The training and testing validation approach gives us an estimate on how the models will perform on new data the models haven't seen yet. In order to split my data into training and testing sets I used a random seed and I have decided to make the proportion of split 80% for the training data and 20% for the testing data. Another important factor in splitting the data is identifying what to stratify the split on, which in my case is the response variable "risk". As seen in the exploratory analysis, my response variable "risk" is divided into three categories or classes: low, moderate, and high, however there is not an evenly distributed number of observations of each of these classes. According to the exploratory analysis 30% of the responses are low, 42% of the responses are moderate, and only 28% of the responses are high. This class imbalance may result in my models having low predictive power for accurately classifying high and possibly even low risk areas. Stratifying on the risk variable avoids the possibility of a disproportionate amount of high risk observations ending up in the training or testing sets. After stratifying, both the training and testing sets should have the same 30%, 42%, and 28% distribution of each class.

```{r}
setwd("C:/Users/18586/Desktop/PSTAT 131/PSTAT-131-final-project/models")
load("pandemic_cum.rda")

set.seed(2002)

pandemic_cum <- pandemic_cum %>%
  dplyr::select(-c("prop", "confirmed_cases", "population", "ID_co"))

pandemic_cum_split <- initial_split(pandemic_cum, prop = 0.80, strata = risk)

pandemic_cum_train <- training(pandemic_cum_split)
pandemic_cum_test <- testing(pandemic_cum_split)
```

### K-Fold Cross Validation:

Here we are going to use k-fold cross validation in order to estimate the accuracy of each of our models on new data and in order to tune the models. The folds will also be stratified on the outcome variable risk in order to handle the data imbalance. Cross Validation is better at estimating how well our models will perform on new data than a single validation set because it assesses the performance on multiple folds of the data, therefore validating on different combinations of the data and taking into account the variation present in the data.

```{r}
pandemic_folds <- vfold_cv(pandemic_cum_train, v = 10, strata = risk)  # 10-fold CV
```

### Recipe:

The recipe for each of my models will use all 21 predictor variables outlined in the introduction along with step codes used to center and scale each of the numerical variables in order to have better performance.

```{r}
pandemic_cum_rec <- recipe(risk ~ ., data = pandemic_cum_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_predictors())
```

# Machine Learning Models

In order to make predictions to classify areas into low, moderate, or high risk for Covid-19, I will build and deploy 7 machine learning models each using the same recipe as seen above.
