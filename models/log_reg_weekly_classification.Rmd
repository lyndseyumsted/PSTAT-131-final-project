---
title: "LM, LDA, QDA and Bayes classification on weekly proportion data"
author: "Lyndsey Umsted"
date: "2022-11-05"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Here I will be fitting a linear regression model to the Weekly Log Transformed Proportion Data

Loading necessary libraries
```{r}
library(tidymodels)
library(ISLR) # For the Smarket data set
library(ISLR2) # For the Bikeshare data set
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
tidymodels_prefer()
```

Splitting into training and testing
```{r}
setwd("C:/Users/18586/Desktop/PSTAT 131/PSTAT-131-final-project/models")
load("pandemic_weekly.rda")

set.seed(2002)

pandemic_weekly <- pandemic_weekly %>%
  select(-c("prop", "new_cases", "population", "ID_co"))

pandemic_weekly_split <- initial_split(pandemic_weekly, prop = 0.80, strata = proplog)

pandemic_weekly_train <- training(pandemic_weekly_split)
pandemic_weekly_test <- testing(pandemic_weekly_split)
```


Creating Recipe:
```{r}
pandemic_weekly_rec <- recipe(proplog ~ ., data = pandemic_weekly_train) %>%
  step_dummy(all_nominal_predictors()) 
```


Logistic Regression Model:
```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
```

Workflow:
```{r}
lm_wflow_weekly <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(pandemic_weekly_rec)
```

Fit logistic regression model to training data:
```{r}
log_reg_fit_weekly <- fit(lm_wflow_weekly, pandemic_weekly_train)
```

We can view the model results:

```{r}
log_reg_fit_weekly %>% 
  # This returns the parsnip object:
  extract_fit_parsnip() %>% 
  # Now tidy the linear model object:
  tidy()
```

Generating preicted values for each observation of the training set:
```{r}
predict(log_reg_fit_weekly, new_data = pandemic_weekly_train, type = "prob")
```


```{r}
augment(log_reg_fit_weekly, new_data = pandemic_weekly_train) %>%
  conf_mat(truth = proplog, estimate = .pred_class)
```

```{r}
log_reg_acc <- augment(log_reg_fit_weekly, new_data = pandemic_weekly_train) %>%
  accuracy(truth = proplog, estimate = .pred_class)
log_reg_acc
```



LDA Model:

```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(pandemic_weekly_rec)

lda_fit_weekly <- fit(lda_wkflow, pandemic_weekly_train)
```

### Assessing Performance

This can be done almost exactly the same way:

```{r}
predict(lda_fit_weekly, new_data = pandemic_weekly_train, type = "prob")
```
Confusion Matrix

```{r}
augment(lda_fit_weekly, new_data = pandemic_weekly_train) %>%
  conf_mat(truth = proplog, estimate = .pred_class) 
```

```{r}
lda_acc <- augment(lda_fit_weekly, new_data = pandemic_weekly_train) %>%
  accuracy(truth = proplog, estimate = .pred_class)
lda_acc
```


QDA

```{r, eval=FALSEs}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(pandemic_weekly_rec)

qda_fit <- fit(qda_wkflow, pandemic_weekly_train)
```

### Assessing Performance

And again:

```{r}
predict(qda_fit, new_data = pandemic_weekly_train, type = "prob")
```

We can view a confidence matrix and calculate accuracy on the **training data**:

```{r}
augment(qda_fit, new_data = pandemic_weekly_train) %>%
  conf_mat(truth = proplog, estimate = .pred_class) 
```

```{r}
qda_acc <- augment(qda_fit, new_data = pandemic_weekly_train) %>%
  accuracy(truth = proplog, estimate = .pred_class)
qda_acc
```


Naive Bayes:


```{r}
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(pandemic_weekly_rec)

nb_fit <- fit(nb_wkflow, pandemic_weekly_train)
```

### Assessing Performance

And again:

```{r, message =FALSE}
predict(nb_fit, new_data = pandemic_weekly_train, type = "prob")
```

We can view a confidence matrix and calculate accuracy on the **training data**:

```{r}
augment(nb_fit, new_data = pandemic_weekly_train) %>%
  conf_mat(truth = proplog, estimate = .pred_class) 
```

```{r}
nb_acc <- augment(nb_fit, new_data = pandemic_weekly_train) %>%
  accuracy(truth = proplog, estimate = .pred_class)
nb_acc
```

## Comparing Model Performance

Now we can make a table of the accuracy rates from these four models to choose the model that produced the highest accuracy on the training data:

```{r}
accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, 
                nb_acc$.estimate, qda_acc$.estimate)
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")
results <- tibble(accuracies = accuracies, models = models)
results %>% 
  arrange(-accuracies)
```

## Fitting to Testing Data

Since the QDA model performed slightly better, we'll go ahead and fit it to the testing data. In future weeks, we'll cover how to use cross-validation to try out different values for models' tuning parameters, but for now, this is a general overview of the process.

```{r}
predict(qda_fit, new_data = pandemic_weekly_test, type = "prob")
```

We can view the confusion matrix on the **testing** data:

```{r}
augment(qda_fit, new_data = pandemic_weekly_test) %>%
  conf_mat(truth = proplog, estimate = .pred_class) 
```

We can also look at the **testing** accuracy. Here, we add two other metrics, sensitivity and specificity, out of curiosity:

```{r}
multi_metric <- metric_set(accuracy, sensitivity, specificity)

augment(qda_fit, new_data = pandemic_weekly_test) %>%
  multi_metric(truth = proplog, estimate = .pred_class)
```